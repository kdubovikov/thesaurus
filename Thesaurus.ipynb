{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import OrderedDict\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# TODO move all of this to regular python script\n",
    "\n",
    "f = open(\"cryptonomicon_engl.txt_Ascii.txt\", \"r\", encoding=\"latin-1\")\n",
    "\n",
    "# chapters is a dict with keys that are chapter names and values that are chapter's content\n",
    "# chapters in a book are supposed to be separated by a line with a \"chapter \" keyword in it\n",
    "chapters = OrderedDict()\n",
    "chapter = ''\n",
    "chapter_title = ''\n",
    "\n",
    "line = f.readline().lower()\n",
    "while line:\n",
    "    # next chapter\n",
    "    if 'chapter ' in line:\n",
    "        chapter_title = line\n",
    "        chapters[chapter_title] = chapter\n",
    "        chapter = ''\n",
    "    \n",
    "    chapter += line    \n",
    "    line = f.readline().lower()\n",
    "    \n",
    "# TODO: do the same with \"from wordnik import *\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_chapter(title, text):\n",
    "    \"\"\"\n",
    "    Tokenize all chapters using NLTK\n",
    "    \"\"\"\n",
    "    tokens = nltk.wordpunct_tokenize(text)\n",
    "    tokens = [w.lower() for w in tokens if w.isalpha()]\n",
    "    pos_tokens = nltk.pos_tag(tokens)\n",
    "    return (title, pos_tokens)\n",
    "\n",
    "# Parallel(n_jobs = multiprocessing.cpu_count())(delayed(tokenized_chapters)(title, text) for title, text in chapters.items())\n",
    "pool = Pool()\n",
    "result = pool.starmap(tokenize_chapter, [[title, text] for title, text in chapters.items()])\n",
    "\n",
    "for t in result:\n",
    "    chapters[t[0]] = t[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pos_definition(tokenized_chapters, chapters):\n",
    "    \"\"\"\n",
    "    Creates a dictionary of book's chapters, containing nested dictionary with definitions of\n",
    "    least frequent adjectives ('adj' key), verbs ('verbs' key) and nouns ('nouns' key)\n",
    "    \"\"\"\n",
    "    for title, tokens in chapters.items():\n",
    "        adjectives = nouns = verbs = []\n",
    "        \n",
    "        # split tokens by POS\n",
    "        for word, token in tokens:\n",
    "            if token[0] == \"J\":\n",
    "                adjectives.append((word, token))\n",
    "            elif token[0] == \"N\":\n",
    "                nouns.append((word, token))\n",
    "            elif token[0] == \"V\":\n",
    "                verbs.append((word, token))\n",
    "        \n",
    "        # TODO: reduce to one loop over pos type\n",
    "        least_common_adj = transform_token_list(adjectives, wn.ADJ, 0.04)\n",
    "        least_common_verb = transform_token_list(verbs, wn.VERB, 0.04)\n",
    "        least_common_noun = transform_token_list(verbs, wn.NOUN, 0.04)\n",
    "\n",
    "        adj_dict = create_definition_dict(least_common_adj)\n",
    "        verb_dict = create_definition_dict(least_common_verb)\n",
    "        noun_dict = create_definition_dict(least_common_noun)\n",
    "        \n",
    "        result = {}\n",
    "        result[\"adjs\"] = adj_dict\n",
    "        result[\"verbs\"] = verb_dict\n",
    "        result[\"nouns\"] = noun_dict\n",
    "        \n",
    "        tokenized_chapters[title] = result\n",
    "\n",
    "def transform_token_list(token_list, pos_value, cutoff):\n",
    "    \"\"\"\n",
    "    Deals with necessary data transformations: drops NA values, \n",
    "    flattens nested lists and selects least frequent items based on cutoff\n",
    "    \"\"\"\n",
    "    token_list_freq = nltk.FreqDist(token_list)\n",
    "\n",
    "    token_list_keys = token_list_freq.keys()\n",
    "    tokens = [wn.synsets(w, pos = pos_value) for w, t in token_list_keys]\n",
    "\n",
    "    # drop na\n",
    "    tokens = list(filter(None, tokens))\n",
    "\n",
    "    # flatten list\n",
    "    tokens = [item for sublist in tokens for item in sublist]\n",
    "    tokens = [(item.lemma_names()[0], item.definition()) for item in tokens]\n",
    "\n",
    "    fraction = int(len(tokens) * cutoff)\n",
    "    return tokens[-fraction:]\n",
    "\n",
    "def create_definition_dict(token_list):\n",
    "    \"\"\"\n",
    "    Reduces several definitions of a token to one semicolon-separated definition\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "\n",
    "    for token in token_list:\n",
    "        if token[0] not in result:\n",
    "            result[token[0]] = token[1]\n",
    "        else:\n",
    "            result[token[0]] += \"; \" + token[1]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# find least common adjectives for each chapter and add them to dict\n",
    "tokenized_chapters = OrderedDict()\n",
    "pos_definition(tokenized_chapters, chapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013/Debian)\n",
      " restricted \\write18 enabled.\n",
      "entering extended mode\n",
      "(./Thesaurus.tex\n",
      "LaTeX2e <2011/06/27>\n",
      "Babel <3.9h> and hyphenation patterns for 2 languages loaded.\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/article.cls\n",
      "Document Class: article 2007/10/19 v1.4h Standard LaTeX document class\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/size10.clo))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/fontenc.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/t1enc.def))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/inputenc.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/utf8.def\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/t1enc.dfu)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/ot1enc.dfu)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/omsenc.dfu)))\n",
      "(/usr/share/texmf/tex/latex/lm/lmodern.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/fontenc.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/t1enc.def))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/fontenc.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/t1enc.def))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/fontenc.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/t1enc.def))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/fontenc.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/t1enc.def))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/fontenc.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/t1enc.def))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/fontenc.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/t1enc.def))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/fontenc.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/t1enc.def))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/fontenc.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/t1enc.def))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/fontenc.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/t1enc.def))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/fontenc.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/t1enc.def))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/fontenc.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/t1enc.def))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/fontenc.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/t1enc.def))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/fontenc.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/t1enc.def))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/fontenc.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/t1enc.def))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/fontenc.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/t1enc.def))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/hyperref/hyperref.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/hobsub-hyperref.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/hobsub-generic.sty))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/graphics/keyval.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/ifxetex/ifxetex.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/oberdiek/auxhook.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/oberdiek/kvoptions.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/hyperref/pd1enc.def)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/latexconfig/hyperref.cfg)\n",
      "\n",
      "! LaTeX Error: File `url.sty' not found.\n",
      "\n",
      "Type X to quit or <RETURN> to proceed,\n",
      "or enter new name. (Default extension: sty)\n",
      "\n",
      "Enter file name: \n",
      "! Emergency stop.\n",
      "<read *> \n",
      "         \n",
      "l.5037 \\let\n",
      "           \\HyOrg@url\\url^^M\n",
      "!  ==> Fatal error occurred, no output PDF file produced!\n",
      "Transcript written on Thesaurus.log.\n",
      "\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['pdflatex', '--interaction=nonstopmode', 'Thesaurus.tex']' returned non-zero exit status 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-515ff25caea1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_pdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Thesaurus\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/kernelmode/anaconda3/lib/python3.5/site-packages/pylatex/document.py\u001b[0m in \u001b[0;36mgenerate_pdf\u001b[1;34m(self, filepath, clean, clean_tex, compiler, compiler_args, silent)\u001b[0m\n\u001b[0;32m    167\u001b[0m                 \u001b[1;31m# For all other errors print the output and raise the error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msilent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kernelmode/anaconda3/lib/python3.5/site-packages/pylatex/document.py\u001b[0m in \u001b[0;36mgenerate_pdf\u001b[1;34m(self, filepath, clean, clean_tex, compiler, compiler_args, silent)\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m                 output = subprocess.check_output(command,\n\u001b[1;32m--> 157\u001b[1;33m                                                  stderr=subprocess.STDOUT)\n\u001b[0m\u001b[0;32m    158\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m                 \u001b[1;31m# Use FileNotFoundError when python 2 is dropped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kernelmode/anaconda3/lib/python3.5/subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[1;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[1;32m--> 629\u001b[1;33m                **kwargs).stdout\n\u001b[0m\u001b[0;32m    630\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kernelmode/anaconda3/lib/python3.5/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(input, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    709\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mretcode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m             raise CalledProcessError(retcode, process.args,\n\u001b[1;32m--> 711\u001b[1;33m                                      output=stdout, stderr=stderr)\n\u001b[0m\u001b[0;32m    712\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mCompletedProcess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCalledProcessError\u001b[0m: Command '['pdflatex', '--interaction=nonstopmode', 'Thesaurus.tex']' returned non-zero exit status 1"
     ]
    }
   ],
   "source": [
    "from pylatex import Document, Section, Subsection, Command, Itemize, Enumerate, Description, Command, Package\n",
    "from pylatex.utils import italic, NoEscape\n",
    "\n",
    "def create_subsection(section_name, section_code, doc, token_dict):\n",
    "    \"\"\"\n",
    "    Creates Thesaurus subsection for specified POS\n",
    "    \"\"\"\n",
    "    with doc.create(Subsection(section_name)):\n",
    "            with doc.create(Description()) as desc:\n",
    "                    for name, description in token_dict[section_code].items():\n",
    "                        desc.add_item(name, description)\n",
    "\n",
    "doc = Document(\"Thesaurus\")\n",
    "\n",
    "doc.packages.append(Package(\"hyperref\"))\n",
    "doc.append(Command(\"tableofcontents\"))\n",
    "doc.append(Command(\"clearpage\"))\n",
    "\n",
    "for title, token_dict in tokenized_chapters.items():\n",
    "    with doc.create(Section(title)):\n",
    "        create_subsection(\"Adjectives\", \"adjs\", doc, token_dict)\n",
    "        create_subsection(\"Nouns\", \"nouns\", doc, token_dict)\n",
    "        create_subsection(\"Verbs\", \"verbs\", doc, token_dict)\n",
    "                    \n",
    "\n",
    "doc.generate_pdf(\"Thesaurus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
